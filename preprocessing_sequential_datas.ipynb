{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d7c7411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238c899b",
   "metadata": {},
   "source": [
    "# 일상 데이터[0,1,0](주로 배회)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49b5be7",
   "metadata": {},
   "source": [
    "#### 데이터 모두 하이라이트에 근접했다고 판단하여 필터링 skip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aeed72",
   "metadata": {},
   "source": [
    "#### average pooling 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d416aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디렉토리 경로 설정\n",
    "directory_path = './sequential_datas/walking' \n",
    "\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        filepath = os.path.join(directory_path, filename)\n",
    "\n",
    "        # csv 파일 로드\n",
    "        df = pd.read_csv(filepath)\n",
    "\n",
    "        # frame 번호에 대한 열 추가하기 (0부터 시작)\n",
    "        df['frame_number'] = np.arange(len(df))\n",
    "\n",
    "        # frame 번호를 3으로 나누어 정수 부분만 취하면, 연속된 세 프레임이 같은 그룹으로 묶일 것입니다.\n",
    "        df['group'] = df['frame_number'] // 3\n",
    "\n",
    "        # 각 그룹에 대해 평균값 계산하기\n",
    "        df_averaged = df.groupby('group').mean()\n",
    "\n",
    "        # 결과 저장하기 \n",
    "        output_filepath = filepath.replace('.csv', '_averaged.csv')\n",
    "        df_averaged.to_csv(output_filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb05a300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (1164, 5, 4)\n",
      "Target shape: (1164, 3)\n"
     ]
    }
   ],
   "source": [
    "# 4가지 관절 각도에 대한 열 이름\n",
    "angle_columns = ['elbow', 'shoulder', 'hip', 'knee']\n",
    "\n",
    "# 디렉토리 경로 설정\n",
    "directory_path = './sequential_datas/walking' \n",
    "\n",
    "X_train_list = []\n",
    "y_train_list = []\n",
    "\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith('_averaged.csv'):\n",
    "        filepath = os.path.join(directory_path, filename)\n",
    "\n",
    "        # csv 파일 로드\n",
    "        df = pd.read_csv(filepath)\n",
    "\n",
    "        # 관절 각도 데이터 가져오기\n",
    "        data = df[angle_columns].values\n",
    "\n",
    "        # 5프레임 동안의 관절각도 변화를 포함하는 리스트 생성하기\n",
    "        sequences = [data[i:i+5] for i in range(len(data)-4)]\n",
    "\n",
    "        X_train_list.extend(sequences)\n",
    "        \n",
    "        # 타겟 설정하기 \n",
    "        y_train_list.extend([[0,1,0]] * len(sequences))\n",
    "\n",
    "# 배열 형태로 변환하기\n",
    "X_train = np.array(X_train_list)\n",
    "y_train = np.array(y_train_list)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")  # 출력 결과: (전체 데이터 개수, 시퀀스 길이, feature 개수)\n",
    "print(f\"Target shape: {y_train.shape}\")   # 출력 결과: (전체 데이터 개수, 클래스 개수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf524724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689c4aa9",
   "metadata": {},
   "source": [
    "# 낙상 데이터[1,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b283a61f",
   "metadata": {},
   "source": [
    "## 변화가 미비한 구간에 대한 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1326cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For falling (1).mp4_angles.csv, number of rows changed from 132 to 60\n",
      "For falling (10).mp4_angles.csv, number of rows changed from 104 to 77\n",
      "For falling (11).mp4_angles.csv, number of rows changed from 76 to 49\n",
      "For falling (12).mp4_angles.csv, number of rows changed from 77 to 77\n",
      "For falling (13).mp4_angles.csv, number of rows changed from 65 to 65\n",
      "For falling (14).mp4_angles.csv, number of rows changed from 85 to 52\n",
      "For falling (15).mp4_angles.csv, number of rows changed from 97 to 97\n",
      "For falling (16).mp4_angles.csv, number of rows changed from 114 to 45\n",
      "For falling (17).mp4_angles.csv, number of rows changed from 84 to 84\n",
      "For falling (18).mp4_angles.csv, number of rows changed from 75 to 75\n",
      "For falling (19).mp4_angles.csv, number of rows changed from 130 to 85\n",
      "For falling (2).mp4_angles.csv, number of rows changed from 103 to 103\n",
      "For falling (20).mp4_angles.csv, number of rows changed from 98 to 98\n",
      "For falling (21).mp4_angles.csv, number of rows changed from 133 to 106\n",
      "For falling (22).mp4_angles.csv, number of rows changed from 104 to 104\n",
      "For falling (23).mp4_angles.csv, number of rows changed from 94 to 65\n",
      "For falling (24).mp4_angles.csv, number of rows changed from 136 to 105\n",
      "For falling (25).mp4_angles.csv, number of rows changed from 121 to 121\n",
      "For falling (26).mp4_angles.csv, number of rows changed from 84 to 84\n",
      "For falling (27).mp4_angles.csv, number of rows changed from 72 to 51\n",
      "For falling (28).mp4_angles.csv, number of rows changed from 132 to 100\n",
      "For falling (29).mp4_angles.csv, number of rows changed from 112 to 88\n",
      "For falling (3).mp4_angles.csv, number of rows changed from 115 to 115\n",
      "For falling (30).mp4_angles.csv, number of rows changed from 83 to 83\n",
      "For falling (4).mp4_angles.csv, number of rows changed from 103 to 51\n",
      "For falling (5).mp4_angles.csv, number of rows changed from 157 to 157\n",
      "For falling (6).mp4_angles.csv, number of rows changed from 90 to 90\n",
      "For falling (7).mp4_angles.csv, number of rows changed from 134 to 75\n",
      "For falling (8).mp4_angles.csv, number of rows changed from 115 to 69\n",
      "For falling (9).mp4_angles.csv, number of rows changed from 107 to 107\n"
     ]
    }
   ],
   "source": [
    "# 4가지 관절 각도에 대한 열 이름\n",
    "angle_columns = ['elbow', 'shoulder', 'hip', 'knee']\n",
    "\n",
    "# rolling window 크기 설정\n",
    "window_size = 5\n",
    "\n",
    "# 임계값 설정\n",
    "threshold = 0.5  \n",
    "\n",
    "# 보존할 주변 프레임 수 설정\n",
    "buffer_frames = 5   \n",
    "\n",
    "directory_path = './sequential_datas/falling'\n",
    "\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        filepath = os.path.join(directory_path, filename)\n",
    "\n",
    "        # csv 파일 로드\n",
    "        df = pd.read_csv(filepath)\n",
    "\n",
    "        for col in angle_columns:\n",
    "            # rolling window 적용하여 표준 편차 계산하기\n",
    "            df[f'{col}_std'] = df[col].rolling(window_size).std()\n",
    "\n",
    "        movement_flags = (df[[f'{col}_std' for col in angle_columns]] < threshold).all(axis=1)\n",
    "\n",
    "        for i in range(-buffer_frames, buffer_frames + 1):\n",
    "            movement_flags |= movement_flags.shift(i)\n",
    "\n",
    "        df_movement_preserved = df[~movement_flags]\n",
    "\n",
    "        # 결과 저장하기 \n",
    "        output_filepath = filepath.replace('.csv', '_movement_preserved.csv')\n",
    "        df_movement_preserved.to_csv(output_filepath, index=False)\n",
    "\n",
    "        print(f\"For {filename}, number of rows changed from {len(df)} to {len(df_movement_preserved)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c946ac4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디렉토리 경로 설정\n",
    "directory_path = './sequential_datas/falling' \n",
    "\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        filepath = os.path.join(directory_path, filename)\n",
    "\n",
    "        # csv 파일 로드\n",
    "        df = pd.read_csv(filepath)\n",
    "\n",
    "        # frame 번호에 대한 열 추가하기 (0부터 시작)\n",
    "        df['frame_number'] = np.arange(len(df))\n",
    "\n",
    "        # frame 번호를 3으로 나누어 정수 부분만 취하면, 연속된 세 프레임이 같은 그룹으로 묶일 것입니다.\n",
    "        df['group'] = df['frame_number'] // 3\n",
    "\n",
    "        # 각 그룹에 대해 평균값 계산하기\n",
    "        df_averaged = df.groupby('group').mean()\n",
    "\n",
    "        # 결과 저장하기 \n",
    "        output_filepath = filepath.replace('.csv', '_averaged.csv')\n",
    "        df_averaged.to_csv(output_filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32ba8894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall Training data shape: (736, 5, 4)\n",
      "Fall Target shape: (736, 3)\n"
     ]
    }
   ],
   "source": [
    "# 낙상 데이터가 있는 디렉토리 경로 설정\n",
    "fall_directory_path = './sequential_datas/falling' \n",
    "\n",
    "X_fall_list = []\n",
    "y_fall_list = []\n",
    "\n",
    "for filename in os.listdir(fall_directory_path):\n",
    "    if filename.endswith('_averaged.csv'):\n",
    "        filepath = os.path.join(fall_directory_path, filename)\n",
    "\n",
    "        # csv 파일 로드\n",
    "        df = pd.read_csv(filepath)\n",
    "\n",
    "        # 관절 각도 데이터 가져오기\n",
    "        data = df[angle_columns].values\n",
    "\n",
    "        # 5프레임 동안의 관절각도 변화를 포함하는 리스트 생성하기\n",
    "        sequences = [data[i:i+5] for i in range(len(data)-4)]\n",
    "\n",
    "        X_fall_list.extend(sequences)\n",
    "        \n",
    "        # 타겟 설정하기 ([1,0,0]으로 설정)\n",
    "        y_fall_list.extend([[1,0,0]] * len(sequences))\n",
    "\n",
    "# 배열 형태로 변환하기\n",
    "X_fall_train = np.array(X_fall_list)\n",
    "y_fall_train = np.array(y_fall_list)\n",
    "\n",
    "print(f\"Fall Training data shape: {X_fall_train.shape}\")  # 출력 결과: (낙상 데이터 개수, 시퀀스 길이, feature 개수)\n",
    "print(f\"Fall Target shape: {y_fall_train.shape}\")   # 출력 결과: (낙상 데이터 개수, 클래스 개수)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501d906d",
   "metadata": {},
   "source": [
    "### 기존 train data와 같은 개수만큼 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a169bbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled daily Training data shape: (736, 5, 4)\n",
      "Sampled daily Target shape: (736, 3)\n"
     ]
    }
   ],
   "source": [
    "# 일상데이터에서 무작위로 fall_data_count 개수만큼 샘플 선택하기\n",
    "indices = np.random.choice(X_train.shape[0], X_fall_train.shape[0], replace=False)\n",
    "X_daily_sampled = X_train[indices]\n",
    "y_daily_sampled = y_train[indices]\n",
    "\n",
    "print(f\"Sampled daily Training data shape: {X_daily_sampled.shape}\")  # 출력 결과: (샘플링된 일상데이터의 수량 , 시퀀스 길이 , feature개수 )\n",
    "print(f\"Sampled daily Target shape: {y_daily_sampled.shape}\")   # 출력 결과 : (샘플링된 일상데이터의 수량 , 클래스개수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd61df6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Training data shape: (1472, 5, 4)\n",
      "Combined Target shape: (1472, 3)\n"
     ]
    }
   ],
   "source": [
    "# 기존의 훈련 데이터와 낙상 훈련데이터를 합치기 \n",
    "X_combined = np.concatenate((X_daily_sampled, X_fall_train), axis=0)\n",
    "y_combined = np.concatenate((y_daily_sampled, y_fall_train), axis=0)\n",
    "\n",
    "print(\"Combined Training data shape:\", X_combined.shape)  \n",
    "print(\"Combined Target shape:\", y_combined.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51137616",
   "metadata": {},
   "source": [
    "# 눕는 데이터[0,0,1](기존의 배회를 대체)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3223cfc",
   "metadata": {},
   "source": [
    "## 변화가 미비한 구간에 대한 필터링\n",
    "### 눕는 과정과 누운 후는 구분\n",
    "#### (누운 후나 낙상 후나 모두 구간별 표준편차가 0에 근접할 것으로 추정하므로 다른 구별에 비해 의미가 크게 없다고 판단)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "716b01f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For lying (1).mp4_angles.csv, number of rows changed from 134 to 79\n",
      "For lying (10).mp4_angles.csv, number of rows changed from 125 to 125\n",
      "For lying (11).mp4_angles.csv, number of rows changed from 135 to 135\n",
      "For lying (12).mp4_angles.csv, number of rows changed from 108 to 79\n",
      "For lying (13).mp4_angles.csv, number of rows changed from 184 to 143\n",
      "For lying (14).mp4_angles.csv, number of rows changed from 207 to 207\n",
      "For lying (15).mp4_angles.csv, number of rows changed from 197 to 121\n",
      "For lying (16).mp4_angles.csv, number of rows changed from 192 to 192\n",
      "For lying (17).mp4_angles.csv, number of rows changed from 190 to 158\n",
      "For lying (18).mp4_angles.csv, number of rows changed from 157 to 157\n",
      "For lying (19).mp4_angles.csv, number of rows changed from 166 to 143\n",
      "For lying (2).mp4_angles.csv, number of rows changed from 120 to 120\n",
      "For lying (20).mp4_angles.csv, number of rows changed from 164 to 164\n",
      "For lying (21).mp4_angles.csv, number of rows changed from 174 to 174\n",
      "For lying (22).mp4_angles.csv, number of rows changed from 166 to 142\n",
      "For lying (23).mp4_angles.csv, number of rows changed from 164 to 132\n",
      "For lying (24).mp4_angles.csv, number of rows changed from 216 to 140\n",
      "For lying (25).mp4_angles.csv, number of rows changed from 140 to 140\n",
      "For lying (26).mp4_angles.csv, number of rows changed from 155 to 113\n",
      "For lying (27).mp4_angles.csv, number of rows changed from 203 to 129\n",
      "For lying (28).mp4_angles.csv, number of rows changed from 216 to 115\n",
      "For lying (29).mp4_angles.csv, number of rows changed from 172 to 101\n",
      "For lying (3).mp4_angles.csv, number of rows changed from 118 to 118\n",
      "For lying (30).mp4_angles.csv, number of rows changed from 134 to 79\n",
      "For lying (31).mp4_angles.csv, number of rows changed from 120 to 120\n",
      "For lying (32).mp4_angles.csv, number of rows changed from 118 to 118\n",
      "For lying (33).mp4_angles.csv, number of rows changed from 104 to 104\n",
      "For lying (34).mp4_angles.csv, number of rows changed from 108 to 108\n",
      "For lying (35).mp4_angles.csv, number of rows changed from 109 to 81\n",
      "For lying (36).mp4_angles.csv, number of rows changed from 108 to 108\n",
      "For lying (37).mp4_angles.csv, number of rows changed from 116 to 62\n",
      "For lying (38).mp4_angles.csv, number of rows changed from 108 to 85\n",
      "For lying (39).mp4_angles.csv, number of rows changed from 125 to 125\n",
      "For lying (4).mp4_angles.csv, number of rows changed from 104 to 104\n",
      "For lying (40).mp4_angles.csv, number of rows changed from 135 to 135\n",
      "For lying (41).mp4_angles.csv, number of rows changed from 108 to 79\n",
      "For lying (42).mp4_angles.csv, number of rows changed from 184 to 143\n",
      "For lying (43).mp4_angles.csv, number of rows changed from 207 to 207\n",
      "For lying (44).mp4_angles.csv, number of rows changed from 197 to 197\n",
      "For lying (45).mp4_angles.csv, number of rows changed from 192 to 192\n",
      "For lying (46).mp4_angles.csv, number of rows changed from 190 to 158\n",
      "For lying (47).mp4_angles.csv, number of rows changed from 157 to 157\n",
      "For lying (48).mp4_angles.csv, number of rows changed from 166 to 143\n",
      "For lying (49).mp4_angles.csv, number of rows changed from 164 to 164\n",
      "For lying (5).mp4_angles.csv, number of rows changed from 108 to 108\n",
      "For lying (50).mp4_angles.csv, number of rows changed from 174 to 174\n",
      "For lying (51).mp4_angles.csv, number of rows changed from 166 to 142\n",
      "For lying (52).mp4_angles.csv, number of rows changed from 197 to 121\n",
      "For lying (53).mp4_angles.csv, number of rows changed from 164 to 132\n",
      "For lying (54).mp4_angles.csv, number of rows changed from 216 to 140\n",
      "For lying (55).mp4_angles.csv, number of rows changed from 140 to 140\n",
      "For lying (56).mp4_angles.csv, number of rows changed from 155 to 113\n",
      "For lying (57).mp4_angles.csv, number of rows changed from 203 to 129\n",
      "For lying (58).mp4_angles.csv, number of rows changed from 216 to 115\n",
      "For lying (59).mp4_angles.csv, number of rows changed from 172 to 101\n",
      "For lying (6).mp4_angles.csv, number of rows changed from 109 to 81\n",
      "For lying (60).mp4_angles.csv, number of rows changed from 187 to 187\n",
      "For lying (61).mp4_angles.csv, number of rows changed from 171 to 171\n",
      "For lying (62).mp4_angles.csv, number of rows changed from 194 to 159\n",
      "For lying (7).mp4_angles.csv, number of rows changed from 108 to 108\n",
      "For lying (8).mp4_angles.csv, number of rows changed from 116 to 62\n",
      "For lying (9).mp4_angles.csv, number of rows changed from 108 to 85\n"
     ]
    }
   ],
   "source": [
    "# 4가지 관절 각도에 대한 열 이름\n",
    "angle_columns = ['elbow', 'shoulder', 'hip', 'knee']\n",
    "\n",
    "# rolling window 크기 설정\n",
    "window_size = 5\n",
    "\n",
    "# 임계값 설정\n",
    "threshold = 0.5  \n",
    "\n",
    "# 보존할 주변 프레임 수 설정\n",
    "buffer_frames = 5   \n",
    "\n",
    "directory_path = './sequential_datas/lying'\n",
    "\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        filepath = os.path.join(directory_path, filename)\n",
    "\n",
    "        # csv 파일 로드\n",
    "        df = pd.read_csv(filepath)\n",
    "\n",
    "        for col in angle_columns:\n",
    "            # rolling window 적용하여 표준 편차 계산하기\n",
    "            df[f'{col}_std'] = df[col].rolling(window_size).std()\n",
    "\n",
    "        movement_flags = (df[[f'{col}_std' for col in angle_columns]] < threshold).all(axis=1)\n",
    "\n",
    "        for i in range(-buffer_frames, buffer_frames + 1):\n",
    "            movement_flags |= movement_flags.shift(i)\n",
    "\n",
    "        df_movement_preserved = df[~movement_flags]\n",
    "\n",
    "        # 결과 저장하기 \n",
    "        output_filepath = filepath.replace('.csv', '_movement_preserved.csv')\n",
    "        df_movement_preserved.to_csv(output_filepath, index=False)\n",
    "\n",
    "        print(f\"For {filename}, number of rows changed from {len(df)} to {len(df_movement_preserved)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f4d0a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디렉토리 경로 설정\n",
    "directory_path = './sequential_datas/lying' \n",
    "\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        filepath = os.path.join(directory_path, filename)\n",
    "\n",
    "        # csv 파일 로드\n",
    "        df = pd.read_csv(filepath)\n",
    "\n",
    "        # frame 번호에 대한 열 추가하기 (0부터 시작)\n",
    "        df['frame_number'] = np.arange(len(df))\n",
    "\n",
    "        # frame 번호를 3으로 나누어 정수 부분만 취하면, 연속된 세 프레임이 같은 그룹으로 묶일 것입니다.\n",
    "        df['group'] = df['frame_number'] // 3\n",
    "\n",
    "        # 각 그룹에 대해 평균값 계산하기\n",
    "        df_averaged = df.groupby('group').mean()\n",
    "\n",
    "        # 결과 저장하기 \n",
    "        output_filepath = filepath.replace('.csv', '_averaged.csv')\n",
    "        df_averaged.to_csv(output_filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "706f0107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lying Training data shape: (2459, 5, 4)\n",
      "Lying Target shape: (2459, 3)\n"
     ]
    }
   ],
   "source": [
    "# 눕기 데이터가 있는 디렉토리 경로 설정\n",
    "lying_directory_path = './sequential_datas/lying' \n",
    "\n",
    "X_lying_list = []\n",
    "y_lying_list = []\n",
    "\n",
    "for filename in os.listdir(lying_directory_path):\n",
    "    if filename.endswith('_averaged.csv'):\n",
    "        filepath = os.path.join(lying_directory_path, filename)\n",
    "\n",
    "        # csv 파일 로드\n",
    "        df = pd.read_csv(filepath)\n",
    "\n",
    "        # 관절 각도 데이터 가져오기\n",
    "        data = df[angle_columns].values\n",
    "\n",
    "        # 5프레임 동안의 관절각도 변화를 포함하는 리스트 생성하기\n",
    "        sequences = [data[i:i+5] for i in range(len(data)-4)]\n",
    "\n",
    "        X_lying_list.extend(sequences)\n",
    "        \n",
    "        # 타겟 설정하기 ([0,0,1]으로 설정)\n",
    "        y_lying_list.extend([[0,0,1]] * len(sequences))\n",
    "\n",
    "# 배열 형태로 변환하기\n",
    "X_lying_train = np.array(X_lying_list)\n",
    "y_lying_train = np.array(y_lying_list)\n",
    "\n",
    "print(f\"Lying Training data shape: {X_lying_train.shape}\")  # 출력 결과: (눕기 데이터 개수, 시퀀스 길이 , feature 개수)\n",
    "print(f\"Lying Target shape: {y_lying_train.shape}\")   # 출력 결과 : (눕기 데이터 개수 , 클래스 개수 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c09823f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled daily Training data shape: (736, 5, 4)\n",
      "Sampled daily Target shape: (736, 3)\n"
     ]
    }
   ],
   "source": [
    "# 기존의 훈련 데이터에서 무작위로 X_fall_train.shape[0] 만큼 샘플 선택하기 \n",
    "indices2= np.random.choice(X_lying_train.shape[0], X_fall_train.shape[0], replace=False) \n",
    "X_lying_sampled= X_lying_train[indices2]\n",
    "y_lying_sampled= y_lying_train[indices2]\n",
    "\n",
    "print(f\"Sampled daily Training data shape: {X_lying_sampled.shape}\")  \n",
    "print(f\"Sampled daily Target shape: {y_lying_sampled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df65f752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Combined Training data shape: (2208, 5, 4)\n",
      "Final Combined Target shape: (2208, 3)\n"
     ]
    }
   ],
   "source": [
    "# 기존의 훈련데이터와 또다시 샘플링된 일상데이터 그리고 샘플링된 빗나간 훈련데이터를 합치는 과정 \n",
    "final_X_combine=np.concatenate((X_combined,X_lying_sampled),axis=0) \n",
    "final_y_combine=np.concatenate((y_combined,y_lying_sampled),axis=0) \n",
    "\n",
    "print(\"Final Combined Training data shape:\", final_X_combine.shape)  \n",
    "print(\"Final Combined Target shape:\", final_y_combine.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91bd7d5",
   "metadata": {},
   "source": [
    "# train dataset 생성완료"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aef667d",
   "metadata": {},
   "source": [
    "## train_test_split 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2da88ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (1766, 5, 4)\n",
      "Training labels shape: (1766, 3)\n",
      "Testing data shape: (442, 5, 4)\n",
      "Testing labels shape: (442, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 클래스 라벨 추출\n",
    "y_labels = np.argmax(final_y_combine, axis=1)\n",
    "\n",
    "# train과 test set으로 분할. stratify 옵션으로 클래스 비율 유지\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_X_combine, final_y_combine,\n",
    "                                                    stratify=y_labels,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)\n",
    "\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Training labels shape:\", y_train.shape)\n",
    "print(\"Testing data shape:\", X_test.shape)\n",
    "print(\"Testing labels shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2790c6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.LSTM(units=128, input_shape=(5, 4), return_sequences=True))\n",
    "model.add(keras.layers.BatchNormalization())  # Batch Normalization 추가\n",
    "model.add(keras.layers.LSTM(units=128))\n",
    "model.add(keras.layers.BatchNormalization())  # Batch Normalization 추가\n",
    "model.add(keras.layers.Dense(units=64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01))) \n",
    "model.add(keras.layers.Dense(units=3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "addc8630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "56/56 [==============================] - 3s 15ms/step - loss: 1.5074 - accuracy: 0.6857 - val_loss: 1.6283 - val_accuracy: 0.6380 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 1.2124 - accuracy: 0.7565 - val_loss: 1.3936 - val_accuracy: 0.7081 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 1.0716 - accuracy: 0.7769 - val_loss: 1.2262 - val_accuracy: 0.6765 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.9463 - accuracy: 0.7860 - val_loss: 1.1042 - val_accuracy: 0.7738 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "56/56 [==============================] - 0s 6ms/step - loss: 0.8701 - accuracy: 0.7865 - val_loss: 0.9508 - val_accuracy: 0.7489 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "56/56 [==============================] - 0s 6ms/step - loss: 0.8101 - accuracy: 0.7928 - val_loss: 0.9083 - val_accuracy: 0.7534 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.7466 - accuracy: 0.8035 - val_loss: 0.7910 - val_accuracy: 0.7624 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.6786 - accuracy: 0.8126 - val_loss: 1.1343 - val_accuracy: 0.5566 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "56/56 [==============================] - 0s 6ms/step - loss: 0.6496 - accuracy: 0.8024 - val_loss: 0.7478 - val_accuracy: 0.7851 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "56/56 [==============================] - 0s 6ms/step - loss: 0.6103 - accuracy: 0.8330 - val_loss: 0.6782 - val_accuracy: 0.7851 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.5891 - accuracy: 0.8250 - val_loss: 0.6538 - val_accuracy: 0.8145 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.5422 - accuracy: 0.8313 - val_loss: 0.5869 - val_accuracy: 0.8235 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "56/56 [==============================] - 0s 6ms/step - loss: 0.5143 - accuracy: 0.8488 - val_loss: 0.5663 - val_accuracy: 0.8281 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.5256 - accuracy: 0.8341 - val_loss: 0.6167 - val_accuracy: 0.8054 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.5294 - accuracy: 0.8267 - val_loss: 0.5423 - val_accuracy: 0.8167 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "56/56 [==============================] - 0s 6ms/step - loss: 0.4977 - accuracy: 0.8409 - val_loss: 0.5257 - val_accuracy: 0.8281 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.4700 - accuracy: 0.8477 - val_loss: 0.7096 - val_accuracy: 0.7489 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.4532 - accuracy: 0.8471 - val_loss: 0.5729 - val_accuracy: 0.8213 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.4521 - accuracy: 0.8641 - val_loss: 0.5597 - val_accuracy: 0.8145 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.4418 - accuracy: 0.8550 - val_loss: 0.5988 - val_accuracy: 0.7602 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "56/56 [==============================] - 0s 6ms/step - loss: 0.4126 - accuracy: 0.8703 - val_loss: 0.4919 - val_accuracy: 0.8484 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.4348 - accuracy: 0.8624 - val_loss: 0.4739 - val_accuracy: 0.8552 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.4066 - accuracy: 0.8675 - val_loss: 0.7724 - val_accuracy: 0.6787 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.4092 - accuracy: 0.8754 - val_loss: 0.9439 - val_accuracy: 0.5520 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.3935 - accuracy: 0.8698 - val_loss: 0.5540 - val_accuracy: 0.8077 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.3940 - accuracy: 0.8658 - val_loss: 0.7365 - val_accuracy: 0.6968 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.3847 - accuracy: 0.8732 - val_loss: 0.5644 - val_accuracy: 0.7738 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.3321 - accuracy: 0.9003 - val_loss: 0.4677 - val_accuracy: 0.8281 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.3066 - accuracy: 0.9083 - val_loss: 0.4372 - val_accuracy: 0.8529 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.3073 - accuracy: 0.9134 - val_loss: 0.4209 - val_accuracy: 0.8643 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.2833 - accuracy: 0.9185 - val_loss: 0.4251 - val_accuracy: 0.8575 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "56/56 [==============================] - 0s 6ms/step - loss: 0.2838 - accuracy: 0.9139 - val_loss: 0.4097 - val_accuracy: 0.8620 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.2770 - accuracy: 0.9190 - val_loss: 0.4092 - val_accuracy: 0.8643 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.2853 - accuracy: 0.9128 - val_loss: 0.4054 - val_accuracy: 0.8688 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "56/56 [==============================] - 0s 6ms/step - loss: 0.2802 - accuracy: 0.9207 - val_loss: 0.3966 - val_accuracy: 0.8643 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.2745 - accuracy: 0.9241 - val_loss: 0.4054 - val_accuracy: 0.8643 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.2569 - accuracy: 0.9281 - val_loss: 0.4031 - val_accuracy: 0.8665 - lr: 1.0000e-04\n",
      "Epoch 38/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.2663 - accuracy: 0.9207 - val_loss: 0.3980 - val_accuracy: 0.8597 - lr: 1.0000e-04\n",
      "Epoch 39/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.2629 - accuracy: 0.9258 - val_loss: 0.4116 - val_accuracy: 0.8688 - lr: 1.0000e-04\n",
      "Epoch 40/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.2552 - accuracy: 0.9270 - val_loss: 0.4045 - val_accuracy: 0.8620 - lr: 1.0000e-04\n",
      "Epoch 41/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.2582 - accuracy: 0.9304 - val_loss: 0.3986 - val_accuracy: 0.8665 - lr: 1.0000e-05\n",
      "Epoch 42/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.2525 - accuracy: 0.9287 - val_loss: 0.3945 - val_accuracy: 0.8643 - lr: 1.0000e-05\n",
      "Epoch 43/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.2474 - accuracy: 0.9332 - val_loss: 0.3934 - val_accuracy: 0.8643 - lr: 1.0000e-05\n",
      "Epoch 44/100\n",
      "56/56 [==============================] - 0s 6ms/step - loss: 0.2492 - accuracy: 0.9281 - val_loss: 0.3916 - val_accuracy: 0.8643 - lr: 1.0000e-05\n",
      "Epoch 45/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.2551 - accuracy: 0.9253 - val_loss: 0.3907 - val_accuracy: 0.8643 - lr: 1.0000e-05\n",
      "Epoch 46/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.2471 - accuracy: 0.9275 - val_loss: 0.3913 - val_accuracy: 0.8665 - lr: 1.0000e-05\n",
      "Epoch 47/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.2478 - accuracy: 0.9326 - val_loss: 0.3895 - val_accuracy: 0.8643 - lr: 1.0000e-05\n",
      "Epoch 48/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.2497 - accuracy: 0.9287 - val_loss: 0.3894 - val_accuracy: 0.8665 - lr: 1.0000e-05\n",
      "Epoch 49/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.2455 - accuracy: 0.9304 - val_loss: 0.3897 - val_accuracy: 0.8665 - lr: 1.0000e-05\n",
      "Epoch 50/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.2517 - accuracy: 0.9320 - val_loss: 0.3897 - val_accuracy: 0.8688 - lr: 1.0000e-05\n",
      "Epoch 51/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.2441 - accuracy: 0.9337 - val_loss: 0.3906 - val_accuracy: 0.8643 - lr: 1.0000e-05\n",
      "Epoch 52/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.2509 - accuracy: 0.9287 - val_loss: 0.3913 - val_accuracy: 0.8665 - lr: 1.0000e-05\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 0s 5ms/step - loss: 0.2462 - accuracy: 0.9298 - val_loss: 0.3916 - val_accuracy: 0.8665 - lr: 1.0000e-06\n",
      "Epoch 54/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.2472 - accuracy: 0.9315 - val_loss: 0.3913 - val_accuracy: 0.8643 - lr: 1.0000e-06\n",
      "Epoch 55/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.2423 - accuracy: 0.9309 - val_loss: 0.3913 - val_accuracy: 0.8688 - lr: 1.0000e-06\n",
      "Epoch 56/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.2408 - accuracy: 0.9292 - val_loss: 0.3908 - val_accuracy: 0.8665 - lr: 1.0000e-06\n",
      "Epoch 57/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.2363 - accuracy: 0.9320 - val_loss: 0.3908 - val_accuracy: 0.8688 - lr: 1.0000e-06\n",
      "Epoch 58/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.2507 - accuracy: 0.9281 - val_loss: 0.3909 - val_accuracy: 0.8688 - lr: 1.0000e-07\n"
     ]
    }
   ],
   "source": [
    "# Learning Rate Scheduler 추가\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=5)\n",
    "\n",
    "# Model Checkpointing 추가\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    \"latest_sequential_model.h5\", save_best_only=True)\n",
    "\n",
    "# Early Stopping\n",
    "early_stopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    epochs=100,\n",
    "                    batch_size=32,\n",
    "                    callbacks=[early_stopping, lr_scheduler, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6943e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
